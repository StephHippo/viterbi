__author__ = 'stephaniehippo'
from forward_probability import Forward_Matrix
from backward_probability import Backward_Matrix
from viterbi_matrix import Viterbi_Matrix

# Given a string y (hmm.observations) and a HMM M,
# P(pi_i = s | y) = probability that observations[i] was emitted from state s
# given the observed sequence y
# P(pi_i = s|y) = posterior probability of state s when the emitted sequence y is known

class PosteriorProbability:
    def __init__(self, hidden_markov_model):
        self.hmm = hidden_markov_model
        self.forward_probabilities = None
        self.backward_probabilities = None
        self.viterbi = None
        self.prob_y = 0
        self.posterior_matrix = {}

    def initialize_posterior(self):
        viterbi = Viterbi_Matrix(self.hmm)
        forward = Forward_Matrix(self.hmm)
        forward.calculate_forward_probabilities()
        self.prob_y = forward.calculate_prob_y()
        self.forward_probabilities = forward.forward_matrix
        backward = Backward_Matrix(self.hmm)
        backward.calculate_backward_probabilities()
        self.backward_probabilities = backward.backward_matrix

    def calculate_posterior_probability(self, position, state):
        if self.forward_probabilities == None or self.backward_probabilities==None:
            self.initialize_posterior()
        #What is the probability that the "T" at the seventh position is generated by the biased coin?
        f_state_i=self.forward_probabilities[state][position]
        b_state_i=self.backward_probabilities[state][position]
        posterior = (f_state_i*b_state_i)/self.prob_y
        return posterior
